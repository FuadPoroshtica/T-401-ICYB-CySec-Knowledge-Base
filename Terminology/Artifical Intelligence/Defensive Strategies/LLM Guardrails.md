---
aliases: []
date created: Monday, 8. December 2025, 11:12
date modified: Monday, 8. December 2025, 11:12
---

# LLM Guardrails
> [!definition] LLM Guardrails
> A defensive strategy for LLMs where you use an intermediate layer to filter and validate inputs and outputs.
> - **Input**: Detect jailbreak attempts.
> - **Output**: Scan for PII (Personally Identifiable Information), malicious code, etc.
